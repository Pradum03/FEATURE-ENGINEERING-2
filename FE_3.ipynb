{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b7f17c-89fb-4c6d-a0c8-26d3a109891a",
   "metadata": {},
   "source": [
    "ASSIGNMENT: FE-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575a0d9-1714-45ea-9f91-d6d88b57e7d4",
   "metadata": {},
   "source": [
    "1.  What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its \n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767370dd-e568-45bb-a1ef-53555517f938",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform the values of a numerical dataset into a specified range. It involves scaling the values of the dataset so that they fall within a minimum and maximum value, typically between 0 and 1.\n",
    "\n",
    "To apply Min-Max scaling, we use the following formula for each value in the dataset:\n",
    "\n",
    "X_norm = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "where X is the original value, X_min is the minimum value in the dataset, X_max is the maximum value in the dataset, and X_norm is the scaled value.\n",
    "\n",
    "Min-Max scaling is often used in machine learning to improve the performance of algorithms that are sensitive to the scale of the input features. By scaling the features to a common range, we can avoid features with larger scales dominating the model and potentially leading to biased results.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset containing the heights of people in centimeters. The minimum height in the dataset is 150 cm, and the maximum height is 200 cm. To apply Min-Max scaling to this dataset, we can use the formula above to scale each value:\n",
    "\n",
    "X_norm = (X - 150) / (200 - 150)\n",
    "\n",
    "For example, if a person's height is 170 cm, the scaled value would be:\n",
    "\n",
    "X_norm = (170 - 150) / (200 - 150) = 0.33\n",
    "\n",
    "After scaling all the values in the dataset, they will all fall within the range of 0 to 1, which can be helpful for training machine learning models that are sensitive to the scale of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe5573-5baf-4dc3-a6fd-6f5a75b1bfde",
   "metadata": {},
   "source": [
    "2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? \n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f2ac25-2981-4abb-87e9-f0e48e5d4509",
   "metadata": {},
   "source": [
    "The Unit Vector technique is another data preprocessing technique used for feature scaling, also known as vector normalization. It scales the values of a feature to have a length of 1, while preserving the direction of the data points. This technique is commonly used when we want to ensure that the magnitude of the feature values does not affect the analysis or machine learning model.\n",
    "\n",
    "To apply the Unit Vector technique to a feature, we first calculate the magnitude of the feature vector, which is the square root of the sum of the squared feature values. Then, we divide each feature value by the magnitude to obtain a new set of values that form a vector with a length of 1.\n",
    "\n",
    "Here's the formula for the Unit Vector technique:\n",
    "\n",
    "X_unit = X / ||X||\n",
    "\n",
    "where X is the original feature vector, ||X|| is the magnitude of the feature vector, and X_unit is the new scaled feature vector.\n",
    "\n",
    "Compared to Min-Max scaling, which scales the values of a feature to a specified range, the Unit Vector technique ensures that all features have the same scale and direction. This can be useful in cases where we want to measure similarity or distance between data points, or when we want to apply algorithms that are sensitive to feature magnitude, such as Principal Component Analysis (PCA).\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Suppose we have a dataset containing the weight and height of people. We want to scale the weight feature using the Unit Vector technique. Suppose we have a person with a weight of 70 kg and a height of 180 cm. The original feature vector is [70, 180].\n",
    "\n",
    "To apply the Unit Vector technique, we first calculate the magnitude of the feature vector:\n",
    "\n",
    "||X|| = sqrt(70^2 + 180^2) = 191.5\n",
    "\n",
    "Then, we divide each feature value by the magnitude to obtain the new scaled feature vector:\n",
    "\n",
    "X_unit = [70 / 191.5, 180 / 191.5] = [0.365, 0.935]\n",
    "\n",
    "After scaling the feature vector using the Unit Vector technique, the feature values have been scaled to have a length of 1, while preserving the direction of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80eb99-d4ce-44b5-b87f-124457574201",
   "metadata": {},
   "source": [
    "3.  What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an \n",
    "example to illustrate its application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f76db9-d7b5-4d02-9f68-61ad4cc7a54a",
   "metadata": {},
   "source": [
    "PCA, or Principal Component Analysis, is a technique used in data analysis and machine learning to reduce the dimensionality of a dataset by identifying and removing the correlated features. It is a method for transforming a dataset of potentially correlated variables into a set of uncorrelated variables, called principal components, that explain the maximum amount of variance in the original dataset.\n",
    "\n",
    "PCA works by identifying the directions of maximum variance in the dataset and projecting the data onto these directions to create a new set of features that capture the most important information in the dataset. The first principal component captures the most variance in the data, followed by the second principal component, and so on.\n",
    "\n",
    "PCA can be useful in reducing the dimensionality of a dataset while retaining most of the information in the data. By reducing the number of features, we can simplify the analysis, speed up the computation, and potentially improve the performance of machine learning models.\n",
    "\n",
    "Here's an example to illustrate the application of PCA:\n",
    "\n",
    "Suppose we have a dataset containing the height, weight, and shoe size of people. We want to reduce the dimensionality of the dataset using PCA. First, we standardize the data to have a mean of zero and a standard deviation of one to ensure that all variables have equal importance in the PCA.\n",
    "\n",
    "Next, we apply PCA to the standardized data to identify the principal components that capture the maximum amount of variance in the data. Let's say we find that the first principal component captures 60% of the variance in the data and is a linear combination of the height and weight variables. The second principal component captures 30% of the variance in the data and is a linear combination of the height and shoe size variables.\n",
    "\n",
    "We can use the first two principal components as new features to represent the original data in a lower-dimensional space. These principal components are uncorrelated and capture the most important information in the data. We can then use these new features for further analysis, such as clustering or classification.\n",
    "\n",
    "By reducing the dimensionality of the dataset from three to two, we have simplified the analysis and potentially improved the performance of machine learning models. PCA can be a powerful tool for handling high-dimensional data and extracting meaningful information from complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe04b38-8e48-4751-b40f-e4a5b562cbd4",
   "metadata": {},
   "source": [
    "4.  What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature \n",
    "Extraction? Provide an example to illustrate this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d21e326-3879-4331-87e5-ec0f4504ce56",
   "metadata": {},
   "source": [
    "PCA can be used for feature extraction, which is a technique that involves creating new features from the existing features of a dataset. The goal of feature extraction is to create a smaller set of features that capture the most important information in the data, while reducing the dimensionality of the dataset.\n",
    "\n",
    "PCA is a powerful technique for feature extraction because it identifies the directions of maximum variance in the dataset and projects the data onto these directions to create a new set of features. These new features are called principal components and are uncorrelated and ordered by the amount of variance they capture in the original dataset.\n",
    "\n",
    "The first principal component captures the most variance in the data, followed by the second principal component, and so on. By selecting the top k principal components, we can create a smaller set of features that capture the most important information in the data, while reducing the dimensionality of the dataset from n to k.\n",
    "\n",
    "Here's an example to illustrate the concept of using PCA for feature extraction:\n",
    "\n",
    "Suppose we have a dataset containing images of handwritten digits, and each image is represented as a 28x28 pixel array. Each pixel is a feature, so the original dataset has a dimensionality of 784 (28x28).\n",
    "\n",
    "We want to extract the most important features from the dataset to reduce the dimensionality and improve the performance of a machine learning model for classifying the digits. We apply PCA to the dataset and find that the first 50 principal components capture 80% of the variance in the data.\n",
    "\n",
    "We can use these 50 principal components as new features to represent the original images in a lower-dimensional space. These features are uncorrelated and capture the most important information in the data, such as the orientation and curvature of the digits. We can then use these new features to train a machine learning model to classify the digits.\n",
    "\n",
    "By reducing the dimensionality of the dataset from 784 to 50, we have simplified the analysis and potentially improved the performance of the machine learning model. PCA can be a powerful tool for feature extraction in image processing, signal processing, and other applications where high-dimensional data needs to be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3538ac5-408d-42c2-930f-9436aa6fc836",
   "metadata": {},
   "source": [
    "5. You are working on a project to build a recommendation system for a food delivery service. The dataset \n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to \n",
    "preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b1158-503b-4399-acf2-12f9167f5b18",
   "metadata": {},
   "source": [
    "To use Min-Max scaling to preprocess the data for a recommendation system for a food delivery service, we would first need to standardize the data, which involves transforming each feature to have a mean of zero and a standard deviation of one. This step ensures that all the features have equal importance in the analysis.\n",
    "\n",
    "Once the data is standardized, we can use Min-Max scaling to rescale each feature to a range between 0 and 1. This step is important because some features, such as price, may have a larger range than other features, such as rating. Rescaling the features to a common range between 0 and 1 ensures that they are all treated equally in the recommendation system.\n",
    "\n",
    "Here's how we would use Min-Max scaling to preprocess the data:\n",
    "\n",
    "Standardize the data: We would subtract the mean of each feature from the values and divide by the standard deviation. This step would transform each feature to have a mean of zero and a standard deviation of one.\n",
    "\n",
    "Apply Min-Max scaling: We would apply the following formula to rescale each feature to a range between 0 and 1:\n",
    "\n",
    "x' = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "where x is the original feature value, min(x) is the minimum value of the feature, max(x) is the maximum value of the feature, and x' is the rescaled value.\n",
    "\n",
    "For example, if the original price range is $5 to $50 and the rating range is 1 to 5, we would use Min-Max scaling to rescale the price feature to a range between 0 and 1 and the rating feature to a range between 0 and 1. This step ensures that both features have equal importance in the recommendation system.\n",
    "\n",
    "By using Min-Max scaling to preprocess the data, we can ensure that all the features are treated equally in the recommendation system and that the range of each feature is consistent. This step can help improve the performance of the recommendation system and ensure that the recommendations are based on a fair comparison of all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2973d6fb-e9b7-49c6-884d-f930c34d4841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    Price  Rating  Delivery Time\n",
      "A     20     4.5             30\n",
      "B     10     3.8             20\n",
      "C     15     4.2             25\n",
      "D     30     4.8             35\n",
      "E     25     3.5             40\n",
      "F     12     4.1             22\n",
      "G     18     3.9             28\n",
      "H     22     4.6             32\n",
      "\n",
      "Scaled Data:\n",
      "    Price    Rating  Delivery Time\n",
      "A   0.50  0.769231           0.50\n",
      "B   0.00  0.230769           0.00\n",
      "C   0.25  0.538462           0.25\n",
      "D   1.00  1.000000           0.75\n",
      "E   0.75  0.000000           1.00\n",
      "F   0.10  0.461538           0.10\n",
      "G   0.40  0.307692           0.40\n",
      "H   0.60  0.846154           0.60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create the dataset\n",
    "data = {'A': [20, 4.5, 30],\n",
    "        'B': [10, 3.8, 20],\n",
    "        'C': [15, 4.2, 25],\n",
    "        'D': [30, 4.8, 35],\n",
    "        'E': [25, 3.5, 40],\n",
    "        'F': [12, 4.1, 22],\n",
    "        'G': [18, 3.9, 28],\n",
    "        'H': [22, 4.6, 32]}\n",
    "\n",
    "# Create a dataframe from the dataset\n",
    "df = pd.DataFrame.from_dict(data, orient='index', columns=['Price', 'Rating', 'Delivery Time'])\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Create a new dataframe with the scaled data\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df.columns, index=df.index)\n",
    "\n",
    "# Display the original and scaled data\n",
    "print('Original Data:\\n', df)\n",
    "print('\\nScaled Data:\\n', df_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce2624-9ebc-411b-8824-f37cd4a45f93",
   "metadata": {},
   "source": [
    "6.  You are working on a project to build a model to predict stock prices. The dataset contains many \n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the \n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239e711-e640-402d-a7a3-7a87c8a3e600",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) can be used to reduce the dimensionality of a dataset with many features, such as the one in the stock price prediction project. The idea is to identify the most important features that contribute the most to the variation in the data, and combine them to form a smaller set of new features, called principal components.\n",
    "\n",
    "Here are the steps to use PCA for dimensionality reduction in the stock price prediction project:\n",
    "\n",
    "Standardize the data: Before applying PCA, it's important to standardize the data to ensure that each feature has the same scale. This can be done using techniques such as Min-Max scaling or Standard scaling.\n",
    "\n",
    "Calculate the covariance matrix: The next step is to calculate the covariance matrix of the standardized data. The covariance matrix is a square matrix that shows how the features are related to each other.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: The eigenvectors and eigenvalues of the covariance matrix represent the direction and magnitude of the principal components, respectively. The eigenvectors are calculated by decomposing the covariance matrix using techniques such as Singular Value Decomposition (SVD).\n",
    "\n",
    "Select the principal components: The next step is to select the top k eigenvectors with the largest eigenvalues. These eigenvectors represent the most important features that contribute the most to the variation in the data.\n",
    "\n",
    "Transform the data: The final step is to transform the original data using the selected eigenvectors to form a new set of features, called principal components. Each principal component is a linear combination of the original features, with the coefficients given by the corresponding eigenvector.\n",
    "\n",
    "By reducing the number of features to a smaller set of principal components, the dimensionality of the dataset can be effectively reduced, which can help to improve the performance of machine learning models by reducing the risk of overfitting and increasing the speed of training.\n",
    "\n",
    "In the context of the stock price prediction project, PCA can be used to identify the most important financial and market trend features that contribute the most to the variation in the data, and combine them to form a smaller set of new features, which can be used to build a predictive model. For example, PCA can be used to identify the top 5 principal components that capture the most important financial and market trend features, and use them as input to a machine learning model for stock price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b70cee2-2586-47fc-9467-da4c83725913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (1000, 10)\n",
      "Transformed data shape: (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a sample stock market dataset with 10 features and 1000 data points\n",
    "data = np.random.rand(1000, 10)\n",
    "\n",
    "# Create a DataFrame to hold the data and set column names\n",
    "df = pd.DataFrame(data, columns=['Price', 'Earnings', 'Revenue', 'Profit Margin', 'Debt/Equity Ratio',\n",
    "                                 'Market Cap', 'Volume', 'PE Ratio', 'PEG Ratio', 'Dividend Yield'])\n",
    "\n",
    "# Standardize the data\n",
    "df_std = (df - df.mean()) / df.std()\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_mat = np.cov(df_std.T)\n",
    "\n",
    "# Compute the eigenvectors and eigenvalues\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "# Select the top 3 eigenvectors with the largest eigenvalues\n",
    "top_eig_vecs = eig_vecs[:, :3]\n",
    "\n",
    "# Transform the data using the selected eigenvectors\n",
    "df_pca = pd.DataFrame(np.dot(df_std, top_eig_vecs), columns=['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "# Print the original data shape and the transformed data shape\n",
    "print('Original data shape:', df.shape)\n",
    "print('Transformed data shape:', df_pca.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e72c1d-820b-42f7-bc49-6b01184feb46",
   "metadata": {},
   "source": [
    "7.  For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the \n",
    "values to a range of -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5182d7-a65a-40bf-9133-6887a9128cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "X = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate the minimum and maximum values\n",
    "X_min = np.min(X)\n",
    "X_max = np.max(X)\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "X_scaled = (X - X_min) / (X_max - X_min) * 2 - 1\n",
    "\n",
    "# Print the scaled values\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00116450-acc7-4b4f-b97b-0f2e3062d5a8",
   "metadata": {},
   "source": [
    "8.  For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform \n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1f492-39cf-4829-881d-71e4fb25d4b6",
   "metadata": {},
   "source": [
    "Performing PCA on the given dataset [height, weight, age, gender, blood pressure] can help to extract the most important features and reduce the dimensionality of the dataset.\n",
    "\n",
    "The number of principal components to retain depends on the amount of variance we want to preserve. The goal is to retain as much variance as possible while reducing the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eab50eb-c27f-4352-bf32-80813a40fbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.52284174 -1.16779276  0.31213104]\n",
      " [ 1.2678436   1.39815728  0.13913134]\n",
      " [-2.05088996  0.35409515  0.38872529]\n",
      " [ 3.51125526 -0.13446876 -0.24434151]\n",
      " [-0.67822786 -0.59192415 -0.11448658]\n",
      " [-2.57282279  0.14193324 -0.48115957]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([\n",
    "    [170, 65, 25, 0, 120],\n",
    "    [165, 55, 32, 1, 130],\n",
    "    [180, 75, 40, 0, 140],\n",
    "    [160, 45, 18, 1, 110],\n",
    "    [175, 70, 35, 0, 125],\n",
    "    [185, 80, 45, 0, 135]\n",
    "])\n",
    "\n",
    "# Standardize the dataset\n",
    "data_std = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "covariance_matrix = np.cov(data_std.T)\n",
    "\n",
    "# Calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# Sort the eigenvalues and eigenvectors in descending order\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "# Choose the number of principal components to retain (in this case, 3)\n",
    "n_components = 3\n",
    "\n",
    "# Project the data onto the chosen principal components\n",
    "principal_components = eigenvectors[:, :n_components]\n",
    "data_transformed = np.dot(data_std, principal_components)\n",
    "\n",
    "# Print the transformed data\n",
    "print(data_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1807a9bf-feb3-4c7c-be4c-772d2142e7fc",
   "metadata": {},
   "source": [
    "In this example, we first define the dataset as a NumPy array. We then standardize the dataset using the mean and std functions from NumPy. We calculate the covariance matrix of the standardized dataset using the cov function. We then calculate the eigenvalues and eigenvectors of the covariance matrix using the eig function from NumPy. We sort the eigenvalues and eigenvectors in descending order to obtain the principal components. We choose to retain the first 3 principal components, and project the data onto these components using matrix multiplication. The resulting transformed data is printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a5772-81b8-447e-a419-a8b696fc630b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
